{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0852273",
   "metadata": {},
   "source": [
    "# Title - EDA Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced266e6",
   "metadata": {},
   "source": [
    "## How does series runtime vary over time for tv and movies?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ac1d6a",
   "metadata": {},
   "source": [
    "## Is there a Relationship between Studio,Genre and Rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a106e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162b7aa",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b09ff",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_numerical_analysis(numerical_frame: pd.DataFrame ,column:str, new_index:str):\n",
    "    \n",
    "    numerical_data = numerical_frame.loc[:,column]\n",
    "\n",
    "    mean_val = round(numerical_data.mean(),2)\n",
    "    median_val = round(numerical_data.median(),2)\n",
    "    std_val = round(numerical_data.std(),2)\n",
    "    range_val = round(numerical_data.max() - numerical_data.min(),2)  \n",
    "    iqr_val = round(numerical_data.quantile(0.75)-numerical_data.quantile(0.25),2)\n",
    "    skew_val = round(numerical_data.skew(),2)\n",
    "    kurtosis_val = round(numerical_data.kurtosis(),2)\n",
    "    coefficient_of_variance_val = round(((std_val / mean_val) if mean_val != 0 else 0),2) \n",
    "    mode_val = numerical_data.mode().tolist()    \n",
    "    mode_val_string = \"\"\n",
    "\n",
    "    for _ in mode_val:\n",
    "        mode_val_string += str(format(_, \".2f\")) + \",\"\n",
    "\n",
    "\n",
    "\n",
    "    output_frame = pd.DataFrame(\n",
    "        {\"Mean\": [mean_val],\n",
    "         \"Median\": [median_val],\n",
    "         \"Mode\": [mode_val],\n",
    "         \"Standard Deviation\": [std_val],\n",
    "         \"Range\": [range_val],\n",
    "         \"Inter-Quartile Range\": [iqr_val],\n",
    "         \"Skewness\": [skew_val],\n",
    "         \"Kurtosis\": [kurtosis_val],\n",
    "         \"Coefficient of Variance\": [coefficient_of_variance_val]})\n",
    "    output_frame.index = [new_index]\n",
    "   \n",
    "    return(output_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_outlier_strip(numerical_frame: pd.DataFrame,column:str):\n",
    "    \n",
    "    q1 = np.percentile(numerical_frame[column], 25)\n",
    "    q3 = np.percentile(numerical_frame[column], 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    outlier_mask = (numerical_frame[column] < lower_bound) | (numerical_frame[column] > upper_bound)\n",
    "\n",
    "    numerical_frame_filtered = numerical_frame[~outlier_mask]\n",
    "\n",
    "    return(numerical_frame_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f2884",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Top_Anime_data.csv\")\n",
    "\n",
    "df.columns = [columns.lower().replace(\" \", \"_\") for columns in df.columns]   ### Anti-whitespace pro snake_case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18883d1",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a2cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3964366",
   "metadata": {},
   "source": [
    "filtering df by data type so data inspectionis easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = df.select_dtypes(exclude=\"number\")\n",
    "cat_columns = list(cat.columns)\n",
    "print(f\"The categorical columns of the cat dataframe are :{cat_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = df.select_dtypes(include=\"number\")\n",
    "num_columns = list(num.columns)\n",
    "print(f\"The numerical columns of num dataframe are :{num_columns}\")\n",
    "num.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c33b0d",
   "metadata": {},
   "source": [
    "### Numeric data check 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_non_null_columns =num.notnull().sum()\n",
    "num_non_null_total = num.notnull().sum().sum()\n",
    "num_non_null_percentage_column = num.notnull().sum() / len(num)\n",
    "num_non_null_percentage_total = (num.notnull().sum().sum()) / (num.size)\n",
    "                                                                            # Count of non-missing values per column:\n",
    "\n",
    "num_dup = num.duplicated()\n",
    "num_dup_total = num.duplicated().sum()                                      # One fewer sum as duplicated returns series not a frame\n",
    "num_na_columns = num.isna().sum()             \n",
    "num_na_total = num.isna().sum().sum()\n",
    "\n",
    "print(f\"\"\"\n",
    "number of non-null values in the frame of numeric data: {num_non_null_total},\n",
    "number of non-null values by column: \\n\\n{num_non_null_columns}\\n\n",
    "\n",
    "precentage of non-null values in the frame of numeric data: {num_non_null_percentage_total * 100},\n",
    "precentage of non-null values by column: \\n\\n{num_non_null_percentage_column * 100}\n",
    "      \n",
    "number of duplicates in the frame of numeric data: {num_dup_total},\n",
    "\n",
    "number of null values in the frame of numeric data:{num_na_total},\n",
    "number of null values by column:\\n\\n{num_na_columns}\\n\"\"\")\n",
    "\n",
    "print(f\"The following rows are duplicates: {','.join(num[num_dup].index )}\") # num_dup is a boolean series which acts as a filter condition\n",
    "print(f\"The following columns have null values: {','.join(num_na_columns[num_na_columns > 0].index)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0864cc",
   "metadata": {},
   "source": [
    "### Categoric data check 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81500a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "##.strip() on cat\n",
    "\n",
    "stripped = lambda _: _.strip() if isinstance(_, str) else _\n",
    "cat = cat.map(stripped)\n",
    "\n",
    "##\n",
    "\n",
    "cat_non_null_columns = cat.notnull().sum()\n",
    "cat_non_null_total = cat.notnull().sum().sum()\n",
    "cat_non_null_percentage_column = cat.notnull().sum() / len(cat)\n",
    "cat_non_null_percentage_total = (cat.notnull().sum().sum()) / (cat.size)\n",
    "                                                                            # Count of non-missing values per column:\n",
    "\n",
    "cat_dup = cat.duplicated()\n",
    "cat_dup_total = cat.duplicated().sum()                                      # One fewer sum as duplicated returns series not a frame\n",
    "cat_na_columns = cat.isna().sum()             \n",
    "cat_na_total = cat.isna().sum().sum()\n",
    "\n",
    "#cat_spaced_values_total_old = cat.eq(\" \").sum().sum()                      # Old basic whitespace detector\n",
    "\n",
    "cat_spaced_values_columns = cat.astype(str).apply(lambda _ : _.str.match(r'^\\s*$')).sum() \n",
    "cat_spaced_values_total = cat.astype(str).apply(lambda _ : _.str.match(r'^\\s*$')).sum().sum() \n",
    "\n",
    "# Treats every column as data type string, and for every column applies the check that it is not continuos whitespace using the regex\n",
    "# expression r'^\\s*$' and the str.match() method. ^ indicates the start, \\s*checks for coninuos whitespace, and $indicates the end of the expression. \n",
    "\n",
    "print(f\"\"\"\n",
    "number of non-null values in the frame of categoric data: {cat_non_null_total},\n",
    "number of non-null values by column: \\n\\n{cat_non_null_columns}\n",
    "\n",
    "percentage of non-null values in the frame of categoric data: {cat_non_null_percentage_total * 100},\n",
    "percentage of non-null values by column: \\n\\n{cat_non_null_percentage_column * 100}\n",
    "\n",
    "number of duplicates in the frame of categoric data: {cat_dup_total},\n",
    "\n",
    "number of white space values in the frame of categoric data: {cat_spaced_values_total},\n",
    "number of white space values by column: \\n\\n{cat_spaced_values_columns}\n",
    "\n",
    "number of null values in the frame of categoric data:{cat_na_total},\n",
    "number of null values by column:\\n\\n{cat_na_columns}\\n\"\"\")\n",
    "\n",
    "print(f\"The following rows are duplicates: {','.join(cat[cat_dup].index )}\") # cat_dup is a boolean series which acts as a filter condition\n",
    "print(f\"The following columns have null values: {','.join(cat_na_columns[cat_na_columns > 0].index)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba57a5a",
   "metadata": {},
   "source": [
    "## Data Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0709eadc",
   "metadata": {},
   "source": [
    "### Making episodes column numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c2724",
   "metadata": {},
   "source": [
    "All null values set to zero, the zeros can be dropped later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a171142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"episodes\"] = pd.to_numeric(df[\"episodes\"], errors = \"coerce\").fillna(0).astype(int)             #null values saved as zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6dc5e1",
   "metadata": {},
   "source": [
    "### Air dates and time on air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_last = df[\"aired\"].astype(str).str.split(\" to \", n=1, expand=True)        # first_last is a new dataframe that takes each result from the split as a new column\n",
    "\n",
    "\n",
    "df[\"aired_first\"] = first_last[0].str.strip()\n",
    "df[\"aired_last\"] = (\n",
    "        df[\"aired_first\"].where(df[\"type\"].eq(\"Movie\"),\n",
    "        first_last[1]\n",
    "        .str.strip()\n",
    "        .replace(\"?\", \"Dec 31, 2024\") if first_last.shape[1] > 1 else pd.NA\n",
    "    )\n",
    ")\n",
    "\n",
    "## .partion() caused so many problems, sometimes fancy speciifc functions are not worth it\n",
    "\n",
    "# df[[\"japanese\",\"aired_first\",\"aired_last\"]].head(50)                          # Entry 50 one piece is proof this worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"aired_first\"] = pd.to_datetime(df[\"aired_first\"], format=\"%b %d, %Y\")\n",
    "df[\"aired_last\"] = pd.to_datetime(df[\"aired_last\"], format=\"%b %d, %Y\")\n",
    "\n",
    "df[\"years_on_air\"] = (df[\"aired_last\"] - df[\"aired_first\"]).dt.days / 365\n",
    "\n",
    "# years_on_air needs to filter out zero for distibutions due to how movies has ve been assigned last_aired dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"years_on_air\"][df[\"type\"] == \"Movie\"].isna().value_counts()         #testing movies have same first last aired values ather than Null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f799702",
   "metadata": {},
   "source": [
    "### Studios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c8303",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maybe its better to store studios in list, because we can still access the studios via index. Keeping studios as a list is the only option\n",
    "## as the number of unique values of studio is so much larger than genre, and also we are not planning on running correlation testing on studios. \n",
    "\n",
    "df[\"studios\"] = (\n",
    "    df[\"studios\"]\n",
    "    .str.replace(\"None found\", \"\")      ## Ugly necessity to kill the the world famous \"None found\" studio\n",
    "    .str.replace(\" add some\", \"\")       ## Ugly necessity to kill the the world famous \"None found\"'s sister studio \" add some\"\n",
    "    .str.split(\",\")\n",
    "    .apply(lambda lst: [_.strip() for _ in lst if _ and _.strip()])    #.strip() on an empty string returns False so wont be added to list.\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd6bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "### example way can be used to find indexes that contain a studio\n",
    "\n",
    "mask = df[\"studios\"].apply(lambda _: \"Studio Ghibli\" in _)        \n",
    "df_ghibli = df[mask]\n",
    "\n",
    "\n",
    "ghibli_production_count =  df_ghibli.shape[0]\n",
    "ghibli_production_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62242b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.union(b) returns a set containing all unique elements of a and b. only a need to be a set\n",
    "# so we can use an empty set and df[*studios] (* unpacks studios, so each list is acted on not the series),\n",
    "\n",
    "list_unique_studios = set().union(*df[\"studios\"])\n",
    "list_unique_studios\n",
    "\n",
    "## \"None found\" and \"add some\" are interesting studios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517423b",
   "metadata": {},
   "source": [
    "### Splitting genres into genres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (                             #tmp as we are only doing this to get a genres list\n",
    "    df[\"genres\"]\n",
    "    .astype(str)\n",
    "    .str.split(\",\")\n",
    "    .explode()                      # new rows for each new unique value \n",
    "    .str.strip()                    # hidden whitespace destroyer so string slicing doesn't miscount\n",
    "    .replace({\"nan\": pd.NA})        # gets rid of nan, the only \"genre\" that doesn't repeat so isnt affected the same way by the filter\n",
    "    .dropna())\n",
    "\n",
    "tmp = sorted(tmp.unique().tolist())\n",
    "\n",
    "genres_list = []\n",
    "\n",
    "for _ in tmp:\n",
    "    genres_list.append(_[0:(len(_)//2)]) # Removes the duplicated words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c465f5",
   "metadata": {},
   "source": [
    "### Assigning Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbddb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_assignment(dataframe):\n",
    "    for i in genres_list:\n",
    "        df[i] = df[\"genres\"].str.contains(i)\n",
    "\n",
    "genre_assignment(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668bd6f3",
   "metadata": {},
   "source": [
    "### Replacing null English names with Japanese Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326588d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"name\"] = df[\"english\"].fillna(df[\"japanese\"])\n",
    "\n",
    "### moving name to the front of the dataframe\n",
    "\n",
    "df = df[[\"name\"] + [c for c in df.columns if c != \"name\"]]\n",
    "\n",
    "df[~df[\"name\"].isnull()] # there is one entry that just wont be removed with .dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95232c",
   "metadata": {},
   "source": [
    "### Epsiodes & Runtime (total runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72181e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime(string):\n",
    "\n",
    "    \"Not pretty but does what it needs to, more robust version would require regex, and is less dangerous than eval()\"\n",
    "\n",
    "    string = string.replace(\" per ep.\",\"\")\n",
    "    string = string.split()\n",
    "\n",
    "    num_list = []\n",
    "\n",
    "    for _ in string:\n",
    "        if \"hr.\"in _ or \"min.\" in _:\n",
    "            if \"hr.\" in _:                \n",
    "                num_list.append(int(_.replace(\"hr.\",\"60\")))\n",
    "            if \"min.\" in _: \n",
    "                num_list.append(int(_.replace(\"min.\",\"1\")))\n",
    "        else:\n",
    "            num_list.append(int(_))\n",
    "\n",
    "    term_1 = np.prod(num_list[0:2]) if len(num_list) >= 2 else 0            #all of this bcause np.prod([]) == 1, leading to total runtime off by a hould episode length\n",
    "    term_2 = np.prod(num_list[2:4]) if len(num_list) >= 4 else 0\n",
    "\n",
    "    total_minutes = term_1 + term_2\n",
    "\n",
    "    return(total_minutes)\n",
    "\n",
    "runtime(\"24 min. per ep.\")\n",
    "runtime(\"1 hr. 17 min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"episode_runtime_minutes\"] = df[\"duration\"].apply(runtime)\n",
    "\n",
    "df[\"total_runtime_minutes\"] = df[\"episode_runtime_minutes\"] * df[\"episodes\"]\n",
    "df[[\"episode_runtime_minutes\",\"episodes\",\"total_runtime_minutes\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50edf9",
   "metadata": {},
   "source": [
    "### Testing if can extract year from first airdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56582e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"aired_first\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84375533",
   "metadata": {},
   "source": [
    "## Data Inspection Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc4ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reapplying the snake_case formatting as new columns have been added\n",
    "\n",
    "df.columns = [columns.lower().replace(\" \", \"_\") for columns in df.columns]   ### Anti-whitespace pro snake_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = df.select_dtypes(exclude=\"number\")\n",
    "\n",
    "cat = cat.drop(columns=\"studios\")                                       # only doing this because the category test can not handle series that contain lists\n",
    "\n",
    "cat_columns = list(cat.columns)\n",
    "print(f\"The categorical columns of the cat dataframe are :{cat_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17844a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = df.select_dtypes(include=\"number\")\n",
    "num_columns = list(num.columns)\n",
    "print(f\"The numerical columns of num dataframe are :{num_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c045b",
   "metadata": {},
   "source": [
    "### Numeric Data Check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_non_null_columns =num.notnull().sum()\n",
    "num_non_null_total = num.notnull().sum().sum()\n",
    "num_non_null_percentage_column = num.notnull().sum() / len(num)\n",
    "num_non_null_percentage_total = (num.notnull().sum().sum()) / (num.size)\n",
    "                                                                            # Count of non-missing values per column:\n",
    "\n",
    "num_dup = num.duplicated()\n",
    "num_dup_total = num.duplicated().sum()                                      # One fewer sum as duplicated returns series not a frame\n",
    "num_na_columns = num.isna().sum()             \n",
    "num_na_total = num.isna().sum().sum()\n",
    "\n",
    "print(f\"\"\"\n",
    "number of non-null values in the frame of numeric data: {num_non_null_total},\n",
    "number of non-null values by column: \\n\\n{num_non_null_columns}\\n\n",
    "\n",
    "precentage of non-null values in the frame of numeric data: {num_non_null_percentage_total * 100},\n",
    "precentage of non-null values by column: \\n\\n{num_non_null_percentage_column * 100}\n",
    "      \n",
    "number of duplicates in the frame of numeric data: {num_dup_total},\n",
    "\n",
    "number of null values in the frame of numeric data:{num_na_total},\n",
    "number of null values by column:\\n\\n{num_na_columns}\\n\"\"\")\n",
    "\n",
    "print(f\"The following rows are duplicates: {','.join(num[num_dup].index )}\") # num_dup is a boolean series which acts as a filter condition\n",
    "print(f\"The following columns have null values: {','.join(num_na_columns[num_na_columns > 0].index)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97bd88",
   "metadata": {},
   "source": [
    "### Catergoric Data Check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdf490",
   "metadata": {},
   "outputs": [],
   "source": [
    "##.strip() on cat\n",
    "\n",
    "stripped = lambda _: _.strip() if isinstance(_, str) else _\n",
    "cat = cat.map(stripped)\n",
    "\n",
    "##\n",
    "\n",
    "cat_non_null_columns = cat.notnull().sum()\n",
    "cat_non_null_total = cat.notnull().sum().sum()\n",
    "cat_non_null_percentage_column = cat.notnull().sum() / len(cat)\n",
    "cat_non_null_percentage_total = (cat.notnull().sum().sum()) / (cat.size)\n",
    "                                                                            # Count of non-missing values per column:\n",
    "\n",
    "cat_dup = cat.duplicated()\n",
    "cat_dup_total = cat.duplicated().sum()                                      # One fewer sum as duplicated returns series not a frame\n",
    "cat_na_columns = cat.isna().sum()             \n",
    "cat_na_total = cat.isna().sum().sum()\n",
    "\n",
    "#cat_spaced_values_total_old = cat.eq(\" \").sum().sum()                      # Old basic whitespace detector\n",
    "\n",
    "cat_spaced_values_columns = cat.astype(str).apply(lambda _ : _.str.match(r'^\\s*$')).sum() \n",
    "cat_spaced_values_total = cat.astype(str).apply(lambda _ : _.str.match(r'^\\s*$')).sum().sum() \n",
    "\n",
    "# Treats every column as data type string, and for every column applies the check that it is not continuos whitespace using the regex\n",
    "# expression r'^\\s*$' and the str.match() method. ^ indicates the start, \\s*checks for coninuos whitespace, and $indicates the end of the expression. \n",
    "\n",
    "print(f\"\"\"\n",
    "number of non-null values in the frame of categoric data: {cat_non_null_total},\n",
    "number of non-null values by column: \\n\\n{cat_non_null_columns}\n",
    "\n",
    "percentage of non-null values in the frame of categoric data: {cat_non_null_percentage_total * 100},\n",
    "percentage of non-null values by column: \\n\\n{cat_non_null_percentage_column * 100}\n",
    "\n",
    "number of duplicates in the frame of categoric data: {cat_dup_total},\n",
    "\n",
    "number of white space values in the frame of categoric data: {cat_spaced_values_total},\n",
    "number of white space values by column: \\n\\n{cat_spaced_values_columns}\n",
    "\n",
    "number of null values in the frame of categoric data:{cat_na_total},\n",
    "number of null values by column:\\n\\n{cat_na_columns}\\n\"\"\")\n",
    "\n",
    "print(f\"The following rows are duplicates: {','.join(cat[cat_dup].index )}\") # cat_dup is a boolean series which acts as a filter condition\n",
    "print(f\"The following columns have null values: {','.join(cat_na_columns[cat_na_columns > 0].index)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf0287a",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e114da5",
   "metadata": {},
   "source": [
    "### Dropping useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_cat = [\"description\",\"synonyms\",\"status\",\"premiered\",\"broadcast\",\"demographic\",\"japanese\",\"english\",\"producers\",\"licensors\",\"genres\",\"aired\",\"rating\",\"genres\"]\n",
    "columns_to_drop_num = [\"popularity\",\"members\",\"duration\",\"aired_last\",\"years_on_air\"]\n",
    "columns_to_drop_df = columns_to_drop_cat + columns_to_drop_num\n",
    "\n",
    "df = df.set_index(df[\"rank\"]) \n",
    "df = df.drop(columns = columns_to_drop_df + [\"rank\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f3778",
   "metadata": {},
   "source": [
    "- df for runtime work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebfe272",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = df.copy()\n",
    "\n",
    "anime.info()\n",
    "\n",
    "genres_list = [_.lower().replace(\" \", \"_\") for _ in genres_list] # genres_list needed to be snake_cased aswell\n",
    "anime_to_drop = genres_list + [\"name\",\"studios\",\"score\",\"source\"]                 \n",
    "anime = anime.drop(columns = anime_to_drop)\n",
    "\n",
    "anime.info()\n",
    "display(anime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36436638",
   "metadata": {},
   "source": [
    "- df for genre work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsts = df.copy()                                         #gsts is genre studio score\n",
    "\n",
    "gsts = gsts[~gsts[\"horror\"].isna()]\n",
    "gsts = gsts.drop(columns = [\"name\",\"type\",\"episodes\",\"aired_first\",\"episode_runtime_minutes\",\"total_runtime_minutes\"])        \n",
    "                \n",
    "gsts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ec3b9",
   "metadata": {},
   "source": [
    "### Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2304b5ac",
   "metadata": {},
   "source": [
    "can be done on-the-fly with the iqr_outlier_strip() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43682512",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ecedd",
   "metadata": {},
   "source": [
    "## Q1) How does series runtime vary over time for tv and movies (seperately)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd96371",
   "metadata": {},
   "source": [
    "How does series runtime vary over time for tv and movies (analyse seperately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_tv_runtime = anime[anime[\"type\"] == \"TV\"]\n",
    "\n",
    "\n",
    "# anime_tv_runtime_filtered = iqr_outlier_strip(anime_tv_runtime,\"episode_runtime_minutes\")         # filering out extreme epidsode lengths by iqr\n",
    "anime_tv_runtime_filtered = iqr_outlier_strip(anime_tv_runtime,\"total_runtime_minutes\")             # filtering out extreme runtimes by iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561ffca",
   "metadata": {},
   "source": [
    "### TV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa9bd0",
   "metadata": {},
   "source": [
    "#### Distributions of the frame as a whole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f8484",
   "metadata": {},
   "source": [
    "By outliers we mean outliers in terms of the metric \"total_runtime_minutes\".  \n",
    "\n",
    "Filtering by episode duration does not affect the overall trend. (test this more)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d7662",
   "metadata": {},
   "source": [
    "First let us look at how long an episode usually is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92817bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(anime_tv_runtime_filtered[\"total_runtime_minutes\"] / anime_tv_runtime_filtered[\"episodes\"], bins = 50)\n",
    "plt.title(\"Episode runtime distribution excluding outliers\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(anime_tv_runtime_filtered[\"total_runtime_minutes\"] / anime_tv_runtime_filtered[\"episodes\"], bins = 50)\n",
    "plt.title(\"Episode runtime distribution excluding outliers\")\n",
    "plt.xlim(20,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce305a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (anime_tv_runtime_filtered[\"total_runtime_minutes\"] / anime_tv_runtime_filtered[\"episodes\"]).to_frame()\n",
    "tmp = tmp.rename(columns = {0:\"runtime_per_episode\"})\n",
    "\n",
    "univariate_numerical_analysis(tmp,\"runtime_per_episode\",\"runtime_per_episode\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a19a473",
   "metadata": {},
   "source": [
    "The majority of episodes appear to be in the 23-24 minute range.\n",
    "\n",
    "Now lets have a look at the distibution of epsisode count and after that we will see if total series runtime duration fits with what we observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a360a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(anime_tv_runtime_filtered[\"episodes\"])\n",
    "plt.title(\"Episode Count distribution excluding outliers\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(anime_tv_runtime_filtered[\"episodes\"],binrange=(0, 200))\n",
    "plt.title(\"Episode Count distribution excluding outliers\")\n",
    "plt.xlim(0,45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(anime_tv_runtime_filtered[\"episodes\"], binrange=(0, 200))\n",
    "plt.title(\"Episode Count distribution excluding outliers\")\n",
    "plt.xlim(8,28)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_numerical_analysis(anime_tv_runtime_filtered,\"episodes\",\"number_of_episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76f3c0",
   "metadata": {},
   "source": [
    "Two distinct peaks. When looking at runtime duration we expect to see a similar distribution as episode count is narrowly distributed on two peaks and episode length is narrowly focused on  one peak.  \n",
    "\n",
    "So variance should not play too large a roll in changing the shape of the distibution.\n",
    "\n",
    "The two disitinct peaks for common episode counts represent shows of episode lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2276ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"median series length when episode total < 20\")\n",
    "display (anime[[\"episodes\",\"type\"]].loc[anime[\"type\"] == \"TV\"].loc[anime[\"episodes\"] < 20].groupby(\"type\").median())\n",
    "\n",
    "print(\"\\nmedian series length when episode total between 20 and 30\")\n",
    "display(anime[[\"episodes\",\"type\"]].loc[anime[\"type\"] == \"TV\"].loc[anime[\"episodes\"].between(20,30)].groupby(\"type\").median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f692f2",
   "metadata": {},
   "source": [
    "The Distribution of Total Show Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98162767",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(anime_tv_runtime[\"total_runtime_minutes\"],binwidth = 250)\n",
    "plt.title(\"Total series runtime distribution including outliers\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(anime_tv_runtime_filtered[\"total_runtime_minutes\"],binwidth = 25)\n",
    "plt.title(\"Total series runtime distribution excluding outliers\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = univariate_numerical_analysis(anime_tv_runtime,\"total_runtime_minutes\",\"total_runtime_minutes_unfiltered\")\n",
    "b = univariate_numerical_analysis(anime_tv_runtime_filtered,\"total_runtime_minutes\",\"total_runtime_minutes_filtered\")\n",
    "pd.concat([a,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8b156",
   "metadata": {},
   "source": [
    "#### Distributions By Time Period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8dbac3",
   "metadata": {},
   "source": [
    "Now to compare this ditribution over different periods of time, by using the initial airdate of a show to categorize which 'block of time' it belongs to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccc0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eighties = anime_tv_runtime_filtered[\"total_runtime_minutes\"][anime_tv_runtime_filtered[\"aired_first\"].between(pd.Timestamp(\"1980-01-01\"),pd.Timestamp(\"1989-12-31\"))]\n",
    "nineties = anime_tv_runtime_filtered[\"total_runtime_minutes\"][anime_tv_runtime_filtered[\"aired_first\"].between(pd.Timestamp(\"1990-01-01\"),pd.Timestamp(\"1999-12-31\"))]\n",
    "two_thousands = anime_tv_runtime_filtered[\"total_runtime_minutes\"][anime_tv_runtime_filtered[\"aired_first\"].between(pd.Timestamp(\"2000-01-01\"),pd.Timestamp(\"2009-12-31\"))]\n",
    "twenty_tens = anime_tv_runtime_filtered[\"total_runtime_minutes\"][anime_tv_runtime_filtered[\"aired_first\"].between(pd.Timestamp(\"2010-01-01\"),pd.Timestamp(\"2019-12-31\"))]\n",
    "twenty_twenties = anime_tv_runtime_filtered[\"total_runtime_minutes\"][anime_tv_runtime_filtered[\"aired_first\"].between(pd.Timestamp(\"2020-01-01\"),pd.Timestamp(\"2024-12-31\"))]\n",
    "\n",
    "bin_width = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade7b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(15, 25), sharex=False, sharey=False)\n",
    "\n",
    "sns.histplot(eighties, binwidth = bin_width,ax=axes[0])\n",
    "axes[0].set_xlim(0,1000)\n",
    "axes[0].set_title(\"Total series runtime '1980 - 1989'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(nineties, binwidth = bin_width,ax=axes[1])\n",
    "axes[1].set_xlim(0,1000)\n",
    "axes[1].set_title(\"Total series runtime '1990 - 1999'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(two_thousands, binwidth = bin_width,ax=axes[2])\n",
    "axes[2].set_xlim(0,1000)\n",
    "axes[2].set_title(\"Total series runtime '2000 - 2010'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(twenty_tens, binwidth = bin_width,ax=axes[3])\n",
    "axes[3].set_xlim(0,1000)\n",
    "axes[3].set_title(\"Total series runtime '2010 - 2020'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(twenty_twenties, binwidth = bin_width,ax=axes[4])\n",
    "axes[4].set_xlim(0,1000)\n",
    "axes[4].set_title(\"Total series runtime '2020 - 2024'\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ffb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(15, 25), sharex=False, sharey=False)\n",
    "\n",
    "sns.histplot(eighties, binwidth = bin_width,ax=axes[0])\n",
    "axes[0].set_xlim(0,1000)\n",
    "axes[0].set_ylim(0,70)\n",
    "axes[0].set_title(\"Total series runtime '1980 - 1989'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(nineties, binwidth = bin_width,ax=axes[1])\n",
    "axes[1].set_xlim(0,1000)\n",
    "axes[1].set_ylim(0,70)\n",
    "axes[1].set_title(\"Total series runtime '1990 - 1999'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(two_thousands, binwidth = bin_width,ax=axes[2])\n",
    "axes[2].set_xlim(0,1000)\n",
    "axes[2].set_ylim(0,70)\n",
    "axes[2].set_title(\"Total series runtime '2000 - 2010'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(twenty_tens, binwidth = bin_width,ax=axes[3])\n",
    "axes[3].set_xlim(0,1000)\n",
    "axes[3].set_ylim(0,70)\n",
    "axes[3].set_title(\"Total series runtime '2010 - 2020'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(twenty_twenties, binwidth = bin_width,ax=axes[4])\n",
    "axes[4].set_xlim(0,1000)\n",
    "axes[4].set_ylim(0,70)\n",
    "axes[4].set_title(\"Total series runtime '2020 - 2024'\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65852361",
   "metadata": {},
   "source": [
    "The twin-peaked nature of the distribution makes a lot of these metrics (skewness,kurtosis,standard deviation for example) redundant. To gain insight from these would require us to investigate each peak seperately which would involve truncation which poses its own issues. However the change in some of these metrics can be sen graphically. We will look at this after a decade by decade comparison of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bce299",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(eighties, binwidth = bin_width, alpha=0.5, label=\"1980s\")\n",
    "sns.histplot(nineties, binwidth = bin_width, alpha=0.5, label=\"1990s\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"1980s Vs. 1990s\")\n",
    "\n",
    "#\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(nineties, binwidth = bin_width, alpha=0.5, label=\"1990s\")\n",
    "sns.histplot(two_thousands, binwidth = bin_width, alpha=0.5, label=\"2000s\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"1990s Vs. 2000s\")\n",
    "\n",
    "#\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(two_thousands, binwidth = bin_width, alpha=0.5, label=\"2000s\")\n",
    "sns.histplot(twenty_tens, binwidth = bin_width, alpha=0.5, label=\"2010s\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"2000s Vs. 2010s\")\n",
    "\n",
    "#\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(twenty_tens, binwidth = bin_width, alpha=0.5, label=\"2010s\")\n",
    "sns.histplot(twenty_twenties, binwidth = bin_width, alpha=0.5, label=\"2020 - 2024\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title(\"2010s Vs. 2020-2024\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(eighties, binwidth = bin_width, alpha=0.5, label=\"1980s\")\n",
    "sns.histplot(nineties, binwidth = bin_width, alpha=0.5, label=\"1990s\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"1980s Vs. 1990s\")\n",
    "plt.ylim(0,70)\n",
    "\n",
    "#\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(nineties, binwidth = bin_width, alpha=0.5, label=\"1990s\")\n",
    "sns.histplot(two_thousands, binwidth = bin_width, alpha=0.5, label=\"2000s\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"1990s Vs. 2000s\")\n",
    "plt.ylim(0,70)\n",
    "\n",
    "#\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(two_thousands, binwidth = bin_width, alpha=0.5, label=\"2000s\")\n",
    "sns.histplot(twenty_tens, binwidth = bin_width, alpha=0.5, label=\"2010s\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"2000s Vs. 2010s\")\n",
    "plt.ylim(0,70)\n",
    "\n",
    "#\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(twenty_tens, binwidth = bin_width, alpha=0.5, label=\"2010s\")\n",
    "sns.histplot(twenty_twenties, binwidth = bin_width, alpha=0.5, label=\"2020 - 2024\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"2010s Vs. 2020-2024\")\n",
    "plt.ylim(0,70)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = univariate_numerical_analysis(anime_tv_runtime_filtered,\"total_runtime_minutes\",\"total_runtime_minutes_overall\")\n",
    "b = univariate_numerical_analysis(eighties.to_frame(),\"total_runtime_minutes\",\"total_runtime_minutes_1980s\")\n",
    "c = univariate_numerical_analysis(nineties.to_frame(),\"total_runtime_minutes\",\"total_runtime_minutes_1990s\")\n",
    "d = univariate_numerical_analysis(two_thousands.to_frame(),\"total_runtime_minutes\",\"total_runtime_minutes_2000s\")\n",
    "e = univariate_numerical_analysis(twenty_tens.to_frame(),\"total_runtime_minutes\",\"total_runtime_minutes_2010s\")\n",
    "f = univariate_numerical_analysis(twenty_twenties.to_frame(),\"total_runtime_minutes\",\"total_runtime_minutes_2020-2024\")\n",
    "pd.concat([a,b,c,d,e,f])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0da8f2",
   "metadata": {},
   "source": [
    "The twin-peaked nature of the distribution makes a lot of these metrics (skewness,kurtosis,standard deviation for example) redundant. To gain insight from these would require us to investigate each peak seperately which would involve truncation which poses its own issues. However the change in some of these metrics can be seen graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade9e59",
   "metadata": {},
   "source": [
    "Worth noting:  \n",
    "- This is not an overall anime trend or even a trend of anime popular in its own time. This is a series of runtime distibutions of popular anime by 2024 standards.  \n",
    "- There is a very low sample volume for the 1980s and 1990s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca60ad",
   "metadata": {},
   "source": [
    "The Trend:\n",
    "\n",
    "- There does appear to some evidence (if weak) to suggest that by 2024 standards, popular (in the top 1000 rated) modern anime tends to be shorter in total run time compared to older popular anime (again by 2024 standards). The location of the peaks and how the lower one becomes dominant suggests that a favouring of lower episode count is the cause.\n",
    "\n",
    "- The twin peak nature of run time caused by episode count appears to start in the 1990s. Favouring the higher episode counts. with each coming decade this dominace fades in  favour of the lower peak. The favouring of the lower peak can be seen in the mean, median, IQR, and standard deviation statisitics dropping over the decades as the lower peak becomes favoured to the point of being the only real peak in the first half of this decade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c3056",
   "metadata": {},
   "source": [
    "### Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967149c9",
   "metadata": {},
   "source": [
    "#### Distributions of the frame as a whole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79a554",
   "metadata": {},
   "source": [
    "By outliers we mean outliers in terms of the metric \"total_runtime_minutes\".  \n",
    "\n",
    "Filtering by film duration does not affect the overall trend. (test this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_movie_runtime = anime[(anime[\"type\"] == \"Movie\") & (anime[\"episodes\"] == 1)]\n",
    "\n",
    "#anime_movie_runtime_filtered = iqr_outlier_strip(anime_movie_runtime,\"episode_runtime_minutes\") \n",
    "anime_movie_runtime_filtered = iqr_outlier_strip(anime_movie_runtime,\"total_runtime_minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc9a38",
   "metadata": {},
   "source": [
    "First let us look at how long movie usually is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.displot(anime_movie_runtime[\"total_runtime_minutes\"] / anime_movie_runtime[\"episodes\"], height = 5, aspect = 2)\n",
    "plt.title(\"Movie runtime distribution including outliers\")\n",
    "\n",
    "sns.displot(anime_movie_runtime_filtered[\"total_runtime_minutes\"] / anime_movie_runtime_filtered[\"episodes\"],height = 5, aspect = 2)\n",
    "plt.title(\"Movie runtime distribution excluding outliers\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace14c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (anime_movie_runtime_filtered[\"total_runtime_minutes\"] / anime_movie_runtime_filtered[\"episodes\"]).to_frame()\n",
    "tmp = tmp.rename(columns = {0:\"film_runtime\"})\n",
    "\n",
    "univariate_numerical_analysis(tmp,\"film_runtime\",\"film_runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25a6fc",
   "metadata": {},
   "source": [
    "#### Distributions By Time Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "eighties_movie = anime_movie_runtime_filtered[\"total_runtime_minutes\"][anime_movie_runtime_filtered[\"aired_first\"].between(pd.Timestamp(\"1980-01-01\"),pd.Timestamp(\"1989-12-31\"))]\n",
    "nineties_movie = anime_movie_runtime_filtered[\"total_runtime_minutes\"][anime_movie_runtime_filtered[\"aired_first\"].between(pd.Timestamp(\"1990-01-01\"),pd.Timestamp(\"1999-12-31\"))]\n",
    "two_thousands_movie = anime_movie_runtime_filtered[\"total_runtime_minutes\"][anime_movie_runtime_filtered[\"aired_first\"].between(pd.Timestamp(\"2000-01-01\"),pd.Timestamp(\"2009-12-31\"))]\n",
    "twenty_tens_movie = anime_movie_runtime_filtered[\"total_runtime_minutes\"][anime_movie_runtime_filtered[\"aired_first\"].between(pd.Timestamp(\"2010-01-01\"),pd.Timestamp(\"2019-12-31\"))]\n",
    "twenty_twenties_movie = anime_movie_runtime_filtered[\"total_runtime_minutes\"][anime_movie_runtime_filtered[\"aired_first\"].between(pd.Timestamp(\"2020-01-01\"),pd.Timestamp(\"2024-12-31\"))]\n",
    "\n",
    "bin_width_movie = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6efd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(15, 25), sharex=True, sharey=True)\n",
    "\n",
    "sns.histplot(eighties_movie, binwidth = bin_width_movie,ax=axes[0])\n",
    "axes[0].set_title(\"Movie runtimes '1980 - 1989'\")\n",
    "axes[0].set_xlim(50,150)\n",
    "axes[0].set_ylim(0,21)\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(nineties_movie, binwidth = bin_width_movie,ax=axes[1])\n",
    "axes[1].set_title(\"Movie runtimes '1990 - 1999'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(two_thousands_movie, binwidth = bin_width_movie,ax=axes[2])\n",
    "\n",
    "axes[2].set_title(\"Movie runtimes '2000 - 2010'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(twenty_tens_movie, binwidth = bin_width_movie,ax=axes[3])\n",
    "axes[3].set_title(\"Movie runtimes '2010 - 2020'\")\n",
    "\n",
    "#\n",
    "\n",
    "sns.histplot(twenty_twenties_movie, binwidth = bin_width_movie,ax=axes[4])\n",
    "axes[4].set_title(\"Movie runtimes '2020 - 2024'\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d010bcc",
   "metadata": {},
   "source": [
    "decade by decade comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(eighties_movie, binwidth = bin_width_movie, alpha=0.5, label=\"1980s\")\n",
    "sns.histplot(nineties_movie, binwidth = bin_width_movie, alpha=0.5, label=\"1990s\")\n",
    "\n",
    "plt.xlim(50,150)\n",
    "plt.ylim(0,17)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"1980s Vs. 1990s\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c03561",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(nineties_movie, binwidth = bin_width_movie, alpha=0.5, label=\"1990s\")\n",
    "sns.histplot(two_thousands_movie, binwidth = bin_width_movie, alpha=0.5, label=\"2000s\")\n",
    "\n",
    "plt.xlim(50,150)\n",
    "plt.ylim(0,17)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"1990s Vs. 2000s\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(two_thousands_movie, binwidth = bin_width_movie, alpha=0.5, label=\"2000s\")\n",
    "sns.histplot(twenty_tens_movie, binwidth = bin_width_movie, alpha=0.5, label=\"2010s\")\n",
    "\n",
    "plt.xlim(50,150)\n",
    "plt.ylim(0,17)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"2000s Vs. 2010s\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb40895",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.histplot(twenty_tens_movie, binwidth = bin_width_movie, alpha=0.5, label=\"2010s\")\n",
    "sns.histplot(twenty_twenties_movie, binwidth = bin_width_movie, alpha=0.5, label=\"2020 - 2024\")\n",
    "\n",
    "plt.xlim(50,150)\n",
    "plt.ylim(0,17)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"2010s Vs. 2020-2024\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = univariate_numerical_analysis(anime_movie_runtime_filtered,\"total_runtime_minutes\",\"total_runtime_minutes_overall\")\n",
    "b = univariate_numerical_analysis(eighties_movie.to_frame(),\"total_runtime_minutes\",\"total_runtime_minutes_1980s\")\n",
    "c = univariate_numerical_analysis(nineties_movie.to_frame(),\"total_runtime_minutes\",\"total_runtime_minutes_1990s\")\n",
    "d = univariate_numerical_analysis(two_thousands_movie.to_frame(),\"total_runtime_minutes\",\"total_runtime_minutes_2000s\")\n",
    "e = univariate_numerical_analysis(twenty_tens_movie.to_frame(),\"total_runtime_minutes\",\"total_runtime_minutes_2010s\")\n",
    "f = univariate_numerical_analysis(twenty_twenties_movie.to_frame(),\"total_runtime_minutes\",\"total_runtime_minutes_2020-2024\")\n",
    "pd.concat([a,b,c,d,e,f])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede69a50",
   "metadata": {},
   "source": [
    "From the plots themselves there does not appear to be any trend apart from a general range of 90-125 minute runtime, and that the 2010 produced a lot of popular (by 2024 standards) anime movies.\n",
    "\n",
    "From the numerical analysis there are small fluctuations, but a lot of this can be explained by low sample volume for any varying period. There is no over-arching trend that is clearly visibile.\n",
    "\n",
    "It is unclear wheter our inability to detect a trend in the variation of movie length distribution is due to low sampling or because it does not exist. All we can say in terms of trends is that we have not been able to find evidence of a one with the current data.\n",
    "\n",
    "However some metrics are stable: mean, median, coefficient of variance.  \n",
    "Some have more variation : standard deviation , iqr\n",
    "and some flucate all over : kurtosis, skewness\n",
    "\n",
    "This suggests a lot of statistical noise around a clear mean and median in a distribution that would benfit from a higher sampling volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c515cea",
   "metadata": {},
   "source": [
    "## Q2) Is there a correlation between Studio,Genre and Rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c13426",
   "metadata": {},
   "source": [
    "average rating-score by genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4334edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_long = gsts.melt(\n",
    "    id_vars=[\"score\"],\n",
    "    value_vars = genres_list,\n",
    "    var_name = \"genre\",\n",
    "    value_name = \"is_genre\"\n",
    ")\n",
    "\n",
    "print(temp_long)\n",
    "\n",
    "temp_long = temp_long[temp_long[\"is_genre\"] == 1]\n",
    "\n",
    "print(temp_long)\n",
    "\n",
    "genre_mean =(\n",
    "    temp_long\n",
    "    .groupby(\"genre\", as_index = False)[\"score\"]\n",
    "    .mean()\n",
    "    .rename(columns = {\"score\" : \"mean_score\"})\n",
    "    .sort_values(\"mean_score\", ascending = False)\n",
    "    .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "genre_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,5))\n",
    "\n",
    "sns.barplot(genre_mean, x=\"genre\", y=\"mean_score\")\n",
    "plt.ylim(7.8,8.3)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title(\"Mean rating score by genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36037be0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsts[\"genre\"] = gsts[genres_list].apply(lambda _: _.index[_].tolist(),axis=1)\n",
    "\n",
    "\n",
    "exploded_studios = (\n",
    "    gsts[[\"studios\",\"score\",\"genre\"]]\n",
    "      .explode('studios'))                               # one row per studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e943c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploded_studios[\"genre\"] = exploded_studios[genres_list].apply(lambda _: _.index[_].tolist(),axis=1)     # genres to list # investigate logic of this again # is boolean masking magic\n",
    "\n",
    "# exploded_studios = exploded_studios.drop(columns = genres_list)\n",
    "\n",
    "top10_studios = exploded_studios[\"studios\"].value_counts().head(10).index\n",
    "\n",
    "exploded_studios_genres = exploded_studios.explode(\"genre\")\n",
    "\n",
    "exploded_studios_genres_grouped = exploded_studios_genres.groupby([\"studios\",\"genre\"]).agg(\"mean\")\n",
    "\n",
    "exploded_studios_genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a198f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_studios_long = exploded_studios_genres[exploded_studios_genres[\"studios\"].isin(top10_studios)]\n",
    "\n",
    "top10_studios_long_grouped = top10_studios_long.groupby([\"studios\",\"genre\"]).agg(\"mean\")\n",
    "\n",
    "top10_studios_long_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a408c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in top10_studios:\n",
    "\n",
    "\n",
    "    studio = _\n",
    "\n",
    "    sub = top10_studios_long_grouped.loc[studio,\"score\"].sort_values(ascending = False)                                 # optional, nice ordering\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(sub.index, sub.values)\n",
    "    plt.title(f\"Average score by genre  {studio}\")\n",
    "    plt.ylabel(\"Average score\")\n",
    "    plt.xlabel(\"Genre\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(7.5,9.0)\n",
    "    plt.show()\n",
    "\n",
    "    #print(sub.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25315d",
   "metadata": {},
   "source": [
    "The fact that studios do not produce all genres does indeed show that there is correlation between genre and studio. The degree of which can be further investigated by seeing the % of total genres the average studios produces.\n",
    "\n",
    "We can also see a relationship between genre and and average rating on a per studio basis. to see overall genre performance through the orginal frame requires different analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_genres = (\n",
    "    gsts[[\"studios\",\"score\",\"genre\"]]\n",
    "      .explode(\"genre\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_genres = exploded_genres[\"genre\"].value_counts().head(5).index\n",
    "\n",
    "exploded_genres_studios = exploded_genres.explode(\"studios\")\n",
    "\n",
    "exploded_genres_studios.groupby([\"genre\",\"studios\"]).agg(\"mean\")\n",
    "\n",
    "# display(top_5_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1dc635",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_genres_long = exploded_genres_studios[exploded_genres_studios[\"genre\"].isin(top_5_genres)]\n",
    "\n",
    "top5_genres_long_grouped = top5_genres_long.groupby([\"genre\",\"studios\"]).agg(\"mean\")\n",
    "\n",
    "top5_genres_long_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in top_5_genres:\n",
    "\n",
    "    genre = _\n",
    "\n",
    "    sub = top5_genres_long_grouped.loc[genre,\"score\"].sort_values(ascending = False)\n",
    "    sub = sub.head(5).sort_values(ascending = False)                                 # optional, nice ordering\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(sub.index, sub.values)\n",
    "    plt.title(f\"Average score by genre  {genre}\")\n",
    "    plt.ylabel(\"Average score\")\n",
    "    plt.xlabel(\"Studio\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(8,9.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de791b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_studios_genres_grouped = exploded_studios_genres.groupby([\"studios\",\"genre\"]).agg(\"mean\").reset_index()\n",
    "plt.figure(figsize = (40,10))\n",
    "sns.heatmap(\n",
    "    exploded_studios_genres_grouped\n",
    "    .pivot(index=\"genre\",columns=\"studios\",values=\"score\"),\n",
    "    cmap=\"crest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc77a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_studios_genres_grouped_10 = exploded_studios_genres_grouped[exploded_studios_genres_grouped[\"score\"]>8.7].sort_values(\"score\", ascending=False)\n",
    "exploded_studios_genres_grouped_10\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(\n",
    "    exploded_studios_genres_grouped_10\n",
    "    .pivot(index=\"genre\",columns=\"studios\",values=\"score\"),\n",
    "    cmap=\"crest\",\n",
    "    annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712c12b",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b1f8f",
   "metadata": {},
   "source": [
    " ## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c83c0",
   "metadata": {},
   "source": [
    " ## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1c7da",
   "metadata": {},
   "source": [
    " ## Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ee935",
   "metadata": {},
   "source": [
    " ## Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84212e0c",
   "metadata": {},
   "source": [
    " ## Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ae178",
   "metadata": {},
   "source": [
    " ## Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35285cc",
   "metadata": {},
   "source": [
    " ## Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e605939",
   "metadata": {},
   "source": [
    " ## Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a57d85",
   "metadata": {},
   "source": [
    " ## Q9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1507f05c",
   "metadata": {},
   "source": [
    " ## Q10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
